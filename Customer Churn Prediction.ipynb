{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74c7b1a",
   "metadata": {},
   "source": [
    "# Sony Research - Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51c8b8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Assignment](#Assignment)\n",
    "* [Data Description](#Data-Description)\n",
    "* [Question 1](#Question-1)\n",
    "* [Question 2](#Question-2)\n",
    "* [Question 3](#Question-3)\n",
    "* [Question 4](#Question-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f5405",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "You are provided with a sample dataset of a telecom companyâ€™s customers and it's expected to done the following tasks:\n",
    "\n",
    "**Question 1:**  Perform exploratory analysis and extract insights from the dataset.\n",
    "\n",
    "**Question 2:** Split the dataset into train/test sets and explain your reasoning.\n",
    "\n",
    "**Question 3:** Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n",
    "\n",
    "**Question 4:** Establish metrics to evaluate model performance.\n",
    "\n",
    "**Question 5:** Discuss the potential issues with deploying the model into production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15283688",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "**State:** The state where a customer comes from\n",
    "\n",
    "**Account length:**\tNumber of days a customer has been using services\n",
    "\n",
    "**Area code:** The area where a customer comes from\n",
    "\n",
    "**Phone number:** The phone number of a customer\n",
    "\n",
    "**International:** The status of customer international plan\n",
    "\n",
    "**Voicemail plan:** The status of customer voicemail plan\n",
    "\n",
    "**Number vmail msgs:** Number of voicemail message sent by a customer\n",
    "\n",
    "**Total day minutes:** Total call minutes spent by a customer during day time\n",
    "\n",
    "**Total day calls:** Total number of calls made by a customer during day time\n",
    "\n",
    "**Total day charge:** Total amount charged to a customer during day time\n",
    "\n",
    "**Total eve minutes:** Total call minutes spent by a customer during evening time\n",
    "\n",
    "**Total eve calls:** Total number of calls made by a customer during evening time\n",
    "\n",
    "**Total eve charge:** Total amount charged to a customer during evening time\n",
    "\n",
    "**Total night minutes:** Total call minutes spent by a customer during night time\n",
    "\n",
    "**Total night calls:** Total number of calls made by a customer during night time\n",
    "\n",
    "**Total night charge:** Total amount charged to a customer during night time\n",
    "\n",
    "**Total intl minutes:** Total international call minutes spent by a customer\n",
    "\n",
    "**Total intl calls:** Total number of international calls made by a customer\n",
    "\n",
    "**Total intl charge:** Total international call amount charged to a customer\n",
    "\n",
    "**Customer service calls:** Total number of customer service calls made by a customer\n",
    "\n",
    "**Churn:** Whether a customer is churned or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8dd2e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Perform exploratory analysis and extract insights from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e089258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66663bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data_Science_Challenge.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a560a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique phone number:' , data['phone number'].nunique())\n",
    "print('Number of unique area code:' , data['area code'].nunique())\n",
    "print('Number of unique state:' , data['state'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba3a1e",
   "metadata": {},
   "source": [
    "Other features such as area code (as long as their distinct value amount does not explode the number of variables in the dataset) can be processed with one-hot encoding to create insight for machine learning models. This effort is necessary because if we would remain them as it is, it could misguide the ML model such as having an implicit ordinal relationship between categories.\n",
    "\n",
    "\n",
    "\n",
    "We will prefer to leave state values out of the dataset in order to not have issues with high dimensionality. We can start to process other categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_code_dummies = pd.get_dummies(data['area code'])\n",
    "\n",
    "# The add_prefix() method inserts the specified value in front of the column label.\n",
    "area_code_dummies = area_code_dummies.add_prefix('area_code_')\n",
    "\n",
    "area_code_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946156b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['voice mail plan'].loc[data['voice mail plan'] == 'no'] = 0\n",
    "\n",
    "data['voice mail plan'].loc[data['voice mail plan'] == 'yes'] = 1\n",
    "\n",
    "data['voice mail plan'] = data['voice mail plan'].astype('int64')\n",
    "\n",
    "data['voice mail plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f74fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['international plan'].loc[data['international plan'] == 'no'] = 0\n",
    "\n",
    "data['international plan'].loc[data['international plan'] == 'yes'] = 1\n",
    "\n",
    "data['international plan'] = data['international plan'].astype('int64')\n",
    "\n",
    "data['international plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data.drop(columns = ['phone number', 'state', 'area code'])\n",
    "\n",
    "#Pandas concat() method is used to concatenate pandas objects such as DataFrames and Series\n",
    "data_final = pd.concat([data_final, area_code_dummies], axis = 1)\n",
    "\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.hist(figsize = (15, 15), bins = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.groupby(['churn'])['churn'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b441a",
   "metadata": {},
   "source": [
    "The distributions tell us:\n",
    "\n",
    "* Most customers don't use voice mail service and international plans.\n",
    "\n",
    "* Half of the customers live in area code 415.\n",
    "\n",
    "* The company earns more by total day calls (check total day charge).\n",
    "\n",
    "* We have an imbalanced dataset which could be tricky when choosing evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d347302",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(23,15))\n",
    "sns.heatmap(data_final.corr(), annot = True, linewidths = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c1d40",
   "metadata": {},
   "source": [
    "From the correlation matrix, we observe the following things:\n",
    "\n",
    "There is a positive correlation between:\n",
    "\n",
    "* total day charge, total day minutes, and churn\n",
    "* total eve minutes and total eve charge\n",
    "* total night minutes and total night charge\n",
    "* total intl minutes and total intl charge\n",
    "* total customer service calls and churn\n",
    "* number vmail messages and voice mail\n",
    "* international plan and churn\n",
    "\n",
    "There is a negative correlation between:\n",
    "\n",
    "* churn and voice mail plan\n",
    "* churn and number vmail messages\n",
    "* churn and total intl calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cee49",
   "metadata": {},
   "source": [
    "Most of the relations are as expected. Still, there could be some interesting points such as the positive correlation between churn and international plan. It could be caused by the poor quality of international plans or calls. Let's check the individual effect of features on churn rate through a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Apply label encoder for churn since its values are also categories.\n",
    "y = label_encoder.fit_transform(data_final['churn'])\n",
    "\n",
    "X = data_final.drop(columns = ['churn'])\n",
    "\n",
    "# Train-test split.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Selected features are selected in multicollinearity check part\n",
    "features_names = [f\"feature {i}\" for i in range((X.shape[1]))]\n",
    "\n",
    "random_forest = RandomForestClassifier(max_depth = 5)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "features = {}  # A dict to hold feature_name : feture_importance\n",
    "\n",
    "# The zip() function returns a zip object, which is an iterator of tuples where the first item in each \n",
    "# passed iterator is paired together, and then the second item in each passed iterator is paired together, etc.\n",
    "for feature,importance in zip(data_final.drop(columns = ['churn']).columns, random_forest.feature_importances_):\n",
    "    features[feature] = importance   # Add the name/value pair\n",
    "    \n",
    "#Feature importance refers to techniques that assign a score to input features based on \n",
    "# how useful they are at predicting a target variable.    \n",
    "    \n",
    "importances = pd.DataFrame.from_dict(features, orient = 'index').rename(columns = {0 : 'Gini-importance'})\n",
    "importances.sort_values(by = 'Gini-importance').plot(kind = 'bar', rot = 90, figsize = (12,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5316ca",
   "metadata": {},
   "source": [
    "Gini-importance shows us which features would be most helpful if we build a tree-based model with given features. According to the analysis above, the most important three churn features are:\n",
    "* total day charge\n",
    "* total day minutes\n",
    "* customer service calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fccf2e",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Split the dataset into train/test sets and explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Apply label encoder for churn since its values are also categories.\n",
    "y = label_encoder.fit_transform(data_final['churn'])\n",
    "\n",
    "X = data_final.drop(columns = ['churn'])\n",
    "\n",
    "# Train-test split.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c8646",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f23145",
   "metadata": {},
   "source": [
    "Since this is a classification problem by definition, we will apply a bunch of classifiers and decide to pick one to use in production based on the performance. Hyperparameters of the given classifiers are chosen as trial-error without applying an advanced hyperparameter tuning mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e54b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"XGBoost\",\n",
    "    \"LightGBM\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "\n",
    "    KNeighborsClassifier(3), \n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    RandomForestClassifier(max_depth=5, random_state=42),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', seed=0),\n",
    "    #LGBMClassifier(random_state=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50b4e7",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Establish metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3d53e",
   "metadata": {},
   "source": [
    "This is a classification task, and the most commonly used metric is accuracy. But, we have an imbalanced dataset, which means we need to be careful about our evaluations. Let's say you have a very skewed dataset with a distribution of 99% of labels 1 and 1% of them 0. Then, if your model always predicts 1, it will have 99% accuracy but still not a good model. F1 score balances the precision and recall so we can have a good metric even for imbalanced datasets. Hence, we will use accuracy and F1 scores while comparing the performance of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b659c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75520b42",
   "metadata": {},
   "source": [
    "## Classical Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941986c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, classification in zip(names,classifiers):\n",
    "    classification.fit(X_train,y_train)\n",
    "    accurate_score = classification.score(X_test, y_test)\n",
    "    y_pred = classification.predict(X_test)\n",
    "    f_score = f1_score(y_test, y_pred, average = 'macro')\n",
    "    print('Accuracy:', \"{:.2f}\".format(accurate_score), \"F1 Score\",\n",
    "          \"{:.2f}\".format(f_score), 'Model:', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159428e0",
   "metadata": {},
   "source": [
    "We obtained good accuracy and satisfying F1 score on tree-based methods. The best performed model is XGBoost with 0.95 accuracy and 0.88 F1-score. Let's visualize the Decision Tree and see how tree-based algorithms decide for our particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG,display,Image\n",
    "import pydotplus\n",
    "\n",
    "# Function attributes\n",
    "# maximum_depth  - depth of tree\n",
    "# criterion_type - [\"gini\" or \"entropy\"]\n",
    "# split_type     - [\"best\" or \"random\"]\n",
    "\n",
    "def plot_decision_tree(maximum_depth, criterion_type, split_type):\n",
    "    clf = DecisionTreeClassifier(max_depth = 3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('Accuracy:', \"{:.2f}\".format(accurate_score), \"F1 Score\",\n",
    "          \"{:.2f}\".format(f_score))\n",
    "    \n",
    "    #plot\n",
    "    graph = tree.export_graphviz(clf, out_file = None, rounded = True,proportion = True,\n",
    "                                feature_names = data_final.drop(columns = ['churn']).columns.to_list(),\n",
    "                                precision  = 2, class_names=[\"Not churn\",\"Churn\"], filled = True,)\n",
    "    \n",
    "    \n",
    "    pydot_graph = pydotplus.graph_from_dot_data(graph)\n",
    "    pydot_graph.set_size('10', '10')\n",
    "    plt = Image(pydot_graph.create_png())\n",
    "    display(plt)\n",
    "    \n",
    "    \n",
    "plot_decision_tree(3, 'gini', 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fc93f",
   "metadata": {},
   "source": [
    "This visualization shows us how the decision tree grows via splitting based on feature values. For example, left-hand side of the tree visualize customers who do not use international plan churns less than who have internal plan. Another example from level two of the tree is customers who call customers service less than 1.47 don't churn while others have higher probability to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb0078",
   "metadata": {},
   "source": [
    "## Deep Learning Model\n",
    "What would be the performance of an Artificial Neural Network (ANN) for the given problem (without spending hours on hyperparameter optimization - just experimenting)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cddd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model to initialise our ann and dense module to build the layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# To have reproducible results\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e56354",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu',\n",
    "                    input_dim = X.shape[1]))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a02bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comiling the ANN | means appliyng SGD on the whole ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n",
    "\n",
    "loss, accuracy = classifier.evaluate(X_train, y_train, batch_size = 10)\n",
    "\n",
    "print('Train accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110af020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print('*'*20)\n",
    "loss, accuracy = classifier.evaluate(X_test, y_test,\n",
    "                            batch_size=10)\n",
    "\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "print('Test F1-score:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2306d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c6935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1511ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7987c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb4596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5f050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccd1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b188166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b8508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
